{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_CACBFsndOCo"
      },
      "source": [
        "# Exercices"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Préliminaires**: Clone de votre repo et imports"
      ],
      "metadata": {
        "id": "hfkMtaHleKAa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! git clone https://github.com/nanopiero/exam_2025.git\n",
        "! cp exam_2025/utils/utils_exercices.py .\n",
        "\n",
        "import copy\n",
        "import numpy as np\n",
        "import torch"
      ],
      "metadata": {
        "id": "xiD_cI-geJjI",
        "outputId": "b0795af4-ac70-4fff-fefb-85eb7f363f0b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'exam_2025'...\n",
            "remote: Enumerating objects: 96, done.\u001b[K\n",
            "remote: Counting objects: 100% (18/18), done.\u001b[K\n",
            "remote: Compressing objects: 100% (11/11), done.\u001b[K\n",
            "remote: Total 96 (delta 11), reused 7 (delta 7), pack-reused 78 (from 2)\u001b[K\n",
            "Receiving objects: 100% (96/96), 3.15 MiB | 8.13 MiB/s, done.\n",
            "Resolving deltas: 100% (32/32), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Clef personnelle pour la partie théorique**\n",
        "\n",
        "Dans la cellule suivante, choisir un entier entre 100 et 1000 (il doit être personnel). Cet entier servira de graine au générateur de nombres aléatoire a conserver pour tous les exercices.\n",
        "\n"
      ],
      "metadata": {
        "id": "J3ga_6BNc5DR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mySeed = 278"
      ],
      "metadata": {
        "id": "PrCTHM4od5UZ"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Remarque** : il est important de respecter la consigne : un choix de graine réellement aléatoire ne doit pas déboucher sur des paires de graines identiques dans une promotions. Les paires identiques sont naturellement suspectées."
      ],
      "metadata": {
        "id": "E0Yv0AmpCgwt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\\\n",
        "\n",
        "---\n",
        "\n",
        "\\"
      ],
      "metadata": {
        "id": "TRWBLVpCWC06"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t5RcggmAkJLV"
      },
      "source": [
        "\\\n",
        "\n",
        "**Exercice 1** *Une relation linéaire*\n",
        "\n",
        "La fonction *generate_dataset* fournit deux jeux de données (entraînement et test). Pour chaque jeu de données, la clef 'inputs' donne accès à un tableau numpy (numpy array) de prédicteurs empilés horizontalement : chaque ligne $i$ contient trois prédicteurs $x_i$, $y_i$ et $z_i$. La clef 'targets' renvoie le vecteur des cibles $t_i$. \\\n",
        "\n",
        "Les cibles sont liées aux prédicteurs par le modèle:\n",
        "$$ t = \\theta_0 + \\theta_1 x + \\theta_2 y + \\theta_3 z + \\epsilon$$ où $\\epsilon \\sim \\mathcal{N}(0,\\eta)$\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from utils_exercices import generate_dataset, Dataset1\n",
        "train_set, test_set = generate_dataset(mySeed)"
      ],
      "metadata": {
        "id": "gEQmgTI8my8i"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_set['targets'].shape"
      ],
      "metadata": {
        "id": "ukEaND76NrR3",
        "outputId": "5eebabc8-9ece-46ad-fbae-a90029f89f43",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(500,)"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q1** Par quelle méthode simple peut-on estimer les coefficients $\\theta_k$ ? La mettre en oeuvre avec la librairie python de votre choix."
      ],
      "metadata": {
        "id": "q5XZTrXNk12K"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "On peut utiliser une régression linéaire, par exemple en utilisant sklearn:\n",
        "\n",
        "**Prompt**: \"A training set and a testing set are stored in python dictionnaries. The train_set dictinnary contain:\n",
        "train_set['inputs']:  horizontally stacked predictands in the form of a numpy array of shape 1000 x 3. and train_set['targets'] : array hor. stacked targets of shape 1000 x 1.\n",
        "The test set only contain 500 lines for both inputs and targets.\n",
        "Please provide a snippet that sole the ML problem by linear regression. Provide estimated coefficients and give MSE on the test set.\""
      ],
      "metadata": {
        "id": "J8t86DUNDTU1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "SDQTIW8iPsEy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Assume train_set and test_set dictionaries are already defined\n",
        "# Extract inputs and targets\n",
        "X_train = train_set['inputs']  # Shape (1000, 3)\n",
        "y_train = train_set['targets']  # Shape (1000, 1)\n",
        "X_test = test_set['inputs']  # Shape (500, 3)\n",
        "y_test = test_set['targets']  # Shape (500, 1)\n",
        "\n",
        "# Create and train the linear regression model\n",
        "model = LinearRegression()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Get estimated coefficients\n",
        "coefficients = model.coef_\n",
        "intercept = model.intercept_\n",
        "print(\"Estimated coefficients:\", coefficients)\n",
        "print(\"Intercept:\", intercept)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Compute Mean Squared Error\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "print(\"Mean Squared Error on test set:\", mse)\n"
      ],
      "metadata": {
        "id": "HITtUqHhFMkn",
        "outputId": "5be706cf-e919-4e1c-86a2-28559dbee40e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Estimated coefficients: [2.73396719 2.9265279  5.584744  ]\n",
            "Intercept: 13.905807493577722\n",
            "Mean Squared Error on test set: 3.5792728778129765\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "MXGXg8tlPULY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q2** Dans les cellules suivantes, on se propose d'estimer les $\\theta_k$ grâce à un réseau de neurones entraîné par SGD. Quelle architecture s'y prête ? Justifier en termes d'expressivité et de performances en généralisation puis la coder dans la cellule suivante."
      ],
      "metadata": {
        "id": "CH_Z5ZEIlQPE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Un perceptron à une couche est suffisant pour exprimer la relation  linéaire entre entrées et cibles. Pour éviter tout surapprentissage, nous prenons pas plus de paramètres que nécessaire (quatre en comptant le biais).\n",
        "\n",
        "prompt:\n",
        "\"please complete these following snippets to train a one layer perceptron with 4 weights (including a bias) through a standard SGD. Provide the weights after the training phase and test the perceptron over the test set. [...]\""
      ],
      "metadata": {
        "id": "7B7PAFycQ4ch"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize model, loss, and optimizer\n",
        "mySimpleNet = SimpleNet()\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = torch.optim.SGD(mySimpleNet.parameters(), lr=0.01)"
      ],
      "metadata": {
        "id": "PPx543blnxdb"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q3** Entraîner cette architecture à la tâche de régression définie par les entrées et sorties du jeu d'entraînement (compléter la cellule ci-dessous)."
      ],
      "metadata": {
        "id": "g6BSTBitpGBD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Training loop\n",
        "num_epochs = 500\n",
        "for epoch in range(num_epochs):\n",
        "    for batch_inputs, batch_targets in dataloader:\n",
        "        # Forward pass\n",
        "        outputs = mySimpleNet(batch_inputs)\n",
        "         # attention : le unsqueeze est nécessaire. Une IA\n",
        "         # ne le fournira pas systématiquement (elle n'a pas compris\n",
        "         # a priori la forme du tenseur batch_targets)\n",
        "        loss = criterion(outputs, batch_targets.unsqueeze(1))\n",
        "\n",
        "        # Backward pass\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()"
      ],
      "metadata": {
        "id": "Wjfa2Z4RoPO-"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q4** Où sont alors stockées les estimations des  $\\theta_k$ ? Les extraire du réseau *mySimpleNet* dans la cellule suivante."
      ],
      "metadata": {
        "id": "OZwKogEEp2Fr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get trained weights\n",
        "trained_weights = mySimpleNet.linear.weight.data\n",
        "trained_bias = mySimpleNet.linear.bias.data\n",
        "print(\"Trained Weights:\", trained_weights.numpy())\n",
        "print(\"Trained Bias:\", trained_bias.numpy())"
      ],
      "metadata": {
        "id": "EjgWp1y1rseb",
        "outputId": "159d987e-ce36-4e6b-82b0-53f199ab2404",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trained Weights: [[2.7327583 2.9275002 5.58484  ]]\n",
            "Trained Bias: [13.905466]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q5** Tester ces estimations sur le jeu de test et comparer avec celles de la question 1. Commentez."
      ],
      "metadata": {
        "id": "pEB-V-oOrJED"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Test the model on test set\n",
        "test_inputs = torch.tensor(test_set['inputs'], dtype=torch.float32)\n",
        "test_targets = torch.tensor(test_set['targets'], dtype=torch.float32)\n",
        "test_outputs = mySimpleNet(test_inputs)\n",
        "\n",
        "# Même remarque qu'au dessus:\n",
        "test_loss = criterion(test_outputs, test_targets.unsqueeze(1)).item()\n",
        "print(\"Test MSE:\", test_loss)\n"
      ],
      "metadata": {
        "id": "cmapedlLUIM6",
        "outputId": "8f2e381c-d31f-4f0d-8a9e-83903b707e1c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test MSE: 3.5797951221466064\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "On a logiquement trouvé des coefficients très légèrement moins bons par SGD que par la méthode \"exacte\" mise en oeuvre (OLS) avec la bibliothèque sklearn."
      ],
      "metadata": {
        "id": "Fq-4DXh1V6_y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\\\n",
        "\n",
        "---\n",
        "\n",
        "\\"
      ],
      "metadata": {
        "id": "VvV2jIrBNtzf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Exercice 2** *Champ réceptif et prédiction causale*"
      ],
      "metadata": {
        "id": "CpRvXCaAtsIN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Le réseau défini dans la cellule suivante est utilisé pour faire le lien entre les valeurs $(x_{t' \\leq t})$ d'une série temporelle d'entrée et la valeur présente $y_t$ d'une série temporelle cible."
      ],
      "metadata": {
        "id": "8JG9wTfK5TBd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from utils_exercices import Outconv, Double_conv_causal, Up_causal, Down_causal\n",
        "\n",
        "\n",
        "class causalFCN(nn.Module):\n",
        "    def __init__(self, dilation=1):\n",
        "        super(causalFCN, self).__init__()\n",
        "        size = 64\n",
        "        n_channels = 1\n",
        "        n_classes = 1\n",
        "        self.inc = Double_conv_causal(n_channels, size)\n",
        "        self.down1 = Down_causal(size, 2*size)\n",
        "        self.down2 = Down_causal(2*size, 4*size)\n",
        "        self.down3 = Down_causal(4*size, 8*size, pooling_kernel_size=5, pooling_stride=5)\n",
        "        self.down4 = Down_causal(8*size, 4*size, pooling=False, dilation=2)\n",
        "        self.up2 = Up_causal(4*size, 2*size, kernel_size=5, stride=5)\n",
        "        self.up3 = Up_causal(2*size, size)\n",
        "        self.up4 = Up_causal(size, size)\n",
        "        self.outc = Outconv(size, n_classes)\n",
        "        self.n_classes = n_classes\n",
        "\n",
        "    def forward(self, x):\n",
        "        x1 = self.inc(x)\n",
        "        x2 = self.down1(x1)\n",
        "        x3 = self.down2(x2)\n",
        "        x4 = self.down3(x3)\n",
        "        x5 = self.down4(x4)\n",
        "        x = self.up2(x5, x3)\n",
        "        x = self.up3(x, x2)\n",
        "        x = self.up4(x, x1)\n",
        "        x = self.outc(x)\n",
        "        return x\n",
        "\n",
        "# Exemple d'utilisation\n",
        "model = causalFCN()\n",
        "# Série temporelle d'entrée (x_t):\n",
        "input_tensor1 = torch.rand(1, 1, 10000)\n",
        "# Série temporelle en sortie f(x_t):\n",
        "output = model(input_tensor1)\n",
        "print(output.shape)"
      ],
      "metadata": {
        "id": "fIbU1EJT1MM9",
        "outputId": "d188f43f-965d-4adf-9f6d-b741d5ccc6c0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 1, 10000])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q1** De quel type de réseau de neurones s'agit-il ? Combien de paramètres la couche self.Down1 compte-t-elle (à faire à la main) ?\n",
        "Combien de paramètres le réseau entier compte-t-il (avec un peu de code) ?"
      ],
      "metadata": {
        "id": "-mNnsYU-7R7N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Il s'agit d'un FCN (Fully Convolutional Network).\n",
        "\n",
        "Pour la suite :\n",
        "\n",
        "**prompt**: \" From the following snippet, could you manually count the number of weights in self.Down1 ? Could you provide a snippet that counts the total number of trainable weights ? Could you also present the first answer (manual count) directly in markdown in order to ease the reading of my notebook ?[...]\"\n",
        "\n",
        "**Remarque**: en plus du contenu de la première cellule de l'exercice, j'ai dû ajouter les définitions de Down_causal et Double_conv_causal dans [...] situées dans utils_exercice (sinon, et l'IA le précise normalement, il y a ambiguïté)."
      ],
      "metadata": {
        "id": "9nltrAlzW0KQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Manually Counting the Number of Weights in self.down1\n",
        "\n",
        "In the causalFCN class, self.down1 is an instance of Down_causal(64, 128). From the Down_causal definition:\n",
        "\n",
        "    If pooling is enabled (which it is by default), Down_causal consists of:\n",
        "        A MaxPool1d layer (which has no trainable parameters).\n",
        "        A Double_conv_causal layer.\n",
        "\n",
        "Now, looking at Double_conv_causal(64, 128), it contains:\n",
        "\n",
        "    First Conv1d layer (conv1)\n",
        "        Input channels: 64\n",
        "        Output channels: 128\n",
        "        Kernel size: 3\n",
        "        Trainable parameters: 128×64×3=24,576128×64×3=24,576\n",
        "        Bias parameters: 128128\n",
        "        Total: 24,704\n",
        "\n",
        "    BatchNorm layer (bn1)\n",
        "        Trainable parameters: 128128 (scaling factor gamma and bias beta).\n",
        "        Total: 128\n",
        "\n",
        "    Second Conv1d layer (conv2)\n",
        "        Input channels: 128\n",
        "        Output channels: 128\n",
        "        Kernel size: 3\n",
        "        Trainable parameters: 128×128×3=49,152128×128×3=49,152\n",
        "        Bias parameters: 128128\n",
        "        Total: 49,280\n",
        "\n",
        "    BatchNorm layer (bn2)\n",
        "        Trainable parameters: 128128\n",
        "        Total: 128\n",
        "\n",
        "Total Number of Trainable Parameters in self.down1\n",
        "24,704+128+49,280+128=74,240\n"
      ],
      "metadata": {
        "id": "5fseBteIYq--"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "# Instantiate the model\n",
        "model = causalFCN()\n",
        "\n",
        "# Count total trainable parameters\n",
        "total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "print(f\"Total trainable parameters: {total_params}\")"
      ],
      "metadata": {
        "id": "qlYxUf6U9vH1",
        "outputId": "7683ab41-5809-4f14-9177-f0229458b70c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total trainable parameters: 2872641\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q2** Par quels mécanismes la taille du vecteur d'entrée est-elle réduite ? Comment est-elle restituée dans la deuxième partie du réseau ?"
      ],
      "metadata": {
        "id": "I4D46A0-8LaV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Réduction de la dimension (spatiale):\n",
        "- utilisation de maxpool1d (Down_causal)\n",
        "  avec noyau de taille 2 ou 5.\n",
        "- paramètre dilation égal à 2.\n",
        "La dimension est restituée par utilisation de couches convtranspose1d (Up_causal).\n",
        "\n",
        "\n",
        "**Remarque**: vous pouviez demander ces mécanismes à une IA. Mais limitez la réponse aux éléments demandés (en copiant collant tout le contenu généré par une IA, vous perdez généralement en concision, et vous êtes alors pénalisés)."
      ],
      "metadata": {
        "id": "8jDH256FaB7b"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q3** Par quels mécanismes le champ réceptif est-il augmenté ? Préciser par un calcul la taille du champ réceptif en sortie de *self.inc*."
      ],
      "metadata": {
        "id": "SVNeFnm88yV2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Le champ réceptif est augmenté par l'effet des convolutions, du maxpooling, du paramètre dilation. Mais les skip connections n'ont pas d'effet, contrairement à ce qui est souvent écrit.\n",
        "\n",
        "Le champ réceptif en sortie de self.inc est de taille cinq (deux convolutions successives de taille avec noyaux de taille trois).\n",
        "\n",
        "**Remarque** : Là encore, les IA donnaient beaucoup plus de détail que nécessaire. Encore une fois, soyez concis."
      ],
      "metadata": {
        "id": "DKWZ2JdZczoa"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "rX1cBN9hYnY8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q4** Par un bout de code, déterminer empiriquement la taille du champ réceptif associé à la composante $y_{5000}$ du vecteur de sortie. \\\\\n",
        "Indices:  \n",
        "- considérer les sorties associées à deux inputs qui ne diffèrent que par une composante...\n",
        "- attention à la mise à jour de paramètres cachés (voire par exemple [track_running_stats](https://pytorch.org/docs/stable/generated/torch.nn.BatchNorm2d.html)) en mode *.train()*.  "
      ],
      "metadata": {
        "id": "TVVcBPuA9EP0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Note**: J'ai ajouté le deuxième indice après la première session de 2025 : seul un étudiant a pensé à passer en mode \"eval\" pour éviter les effets des couches de normalisation lors d'évaluation successives."
      ],
      "metadata": {
        "id": "unVx88PKedbZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "# Set device to GPU if available\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Exemple d'utilisation\n",
        "model = causalFCN().to(device)\n",
        "model.eval()  # Set model to evaluation mode\n",
        "\n",
        "# Série temporelle d'entrée (x_t):\n",
        "input_tensor1 = torch.rand(1, 1, 10000, device=device)\n",
        "\n",
        "# Série temporelle en sortie f(x_t):\n",
        "output = model(input_tensor1)\n",
        "print(output.shape)\n",
        "\n",
        "# Measure receptive field empirically\n",
        "output_index = 5000\n",
        "start_index = None\n",
        "stop_index = None\n",
        "\n",
        "for i in range(10000):\n",
        "    perturbed_input = input_tensor1.clone()\n",
        "    perturbed_input[0, 0, i] += 1e-3  # Small perturbation\n",
        "    perturbed_output = model(perturbed_input)\n",
        "\n",
        "    if torch.abs(perturbed_output[0, 0, output_index] - output[0, 0, output_index]) > 1e-5:\n",
        "        if start_index is None:\n",
        "            start_index = i\n",
        "        stop_index = i\n",
        "\n",
        "print(f\"Receptive field starts at index: {start_index}\")\n",
        "print(f\"Receptive field stops at index: {stop_index}\")\n",
        "print(f\"Receptive field size: {stop_index - start_index + 1}\")"
      ],
      "metadata": {
        "id": "69WMWCSZAg5_",
        "outputId": "8f1058ee-0ed4-4645-812d-a17bb05e9b46",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 1, 10000])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q5** $y_{5000}$ dépend-elle des composantes $x_{t, \\space t > 5000}$ ? Justifier de manière empirique puis préciser la partie du code de Double_conv_causal qui garantit cette propriété de \"causalité\" en justifiant.  \n",
        "\n"
      ],
      "metadata": {
        "id": "gZ37skwm-Vpv"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "PeooRYE-ATGt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\\\n",
        "\n",
        "---\n",
        "\n",
        "\\"
      ],
      "metadata": {
        "id": "qV52tusgNn6A"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\\\n",
        "\n",
        "Exercice 3: \"Ranknet loss\""
      ],
      "metadata": {
        "id": "bm-sRzmfqc2m"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Un [article récent](https://https://arxiv.org/abs/2403.14144) revient sur les progrès en matière de learning to rank. En voilà un extrait :"
      ],
      "metadata": {
        "id": "Wl8wUjsSM57D"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "<img src=\"https://raw.githubusercontent.com/nanopiero/exam_2025/refs/heads/main/utils/png_exercice3.PNG\" alt=\"extrait d'un article\" width=\"800\">"
      ],
      "metadata": {
        "id": "SDZUXMlSDpoe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q1** Qu'est-ce que les auteurs appellent \"positive samples\" et \"negative samples\" ? Donner un exemple."
      ],
      "metadata": {
        "id": "9NzV1PbMNyuo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q2** Dans l'expression de $\\mathcal{L}_{RankNet}$, d'où proviennent les $z_i$ ? Que représentent-ils ?  "
      ],
      "metadata": {
        "id": "yIKQ5Eo9OnPq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q3** Pourquoi cette expression conduit-elle à ce que, après apprentissage, \"the estimated\n",
        "value of positive samples is greater than that of negative samples\n",
        "for each pair of positive/negative samples\" ?"
      ],
      "metadata": {
        "id": "r74fWiyvPb7Q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q4** Dans le cadre d'une approche par deep learning, quels termes utilise-t-on pour qualifier les réseaux de neurones exploités et la modalité suivant laquelle ils sont entraînés ?"
      ],
      "metadata": {
        "id": "pk1EIi_VVi3R"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}